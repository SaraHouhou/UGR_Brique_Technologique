# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cp9Gjp_MiQWyzue66T29F37vkPub5CuY
"""

! pip install h5py pyaml

import sy
from tensorflow.keras.applications.vgg16 import VGG16
from keras.engine.sequential import Sequential
from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, Reshape, UpSampling2D, Activation

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping
import matplotlib.pyplot as plt
print(sys.version)


import tensorflow as tf

"""## Selecting Between Strategies

### TPU or GPU detection

Depending on the hardware available, you'll use different distribution strategies.  For a review on distribution strategies, please check out the second course in this specialization ["Custom and Distributed Training with TensorFlow"](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow), week 4, "Distributed Training".

- If the TPU is available, then you'll be using the TPU Strategy.
Otherwise:
- If more than one GPU is available, then you'll use the Mirrored Strategy
- If one GPU is available or if just the CPU is available, you'll use the default strategy.
"""

# Detect hardware
try:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection
except ValueError:
  tpu = None
  gpus = tf.config.experimental.list_logical_devices("GPU")
    
# Select appropriate distribution strategy
if tpu:
  tf.config.experimental_connect_to_cluster(tpu)
  tf.tpu.experimental.initialize_tpu_system(tpu)
  strategy = tf.distribute.experimental.TPUStrategy(tpu) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.
  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  
elif len(gpus) > 1:
  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])
  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])
elif len(gpus) == 1:
  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU
  print('Running on single GPU ', gpus[0].name)
else:
  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU
  print('Running on CPU')
print("Number of accelerators: ", strategy.num_replicas_in_sync)

# Commented out IPython magic to ensure Python compatibility.
from numpy.core.multiarray import result_type





"""# After Uploading the drive content for training, copy the implemented code 
# First: Load Data Set
For this work, we will use a subset of the `Hagrid dataset` data set, which contains images of 14 gestures, we want to train only 9 clases. 

Download the `traiN` and `valid` sets by running the cell below:

This dataset already has an structure that is compatible with Keras' `flow_from_directory` so I don't need to move the images into subdirectories because I do It before uploading them online.  However, it is still a good idea to save the paths of the images so you can use them later on:
"""

import os

GESTURES_Names = [
    "five",
    "four",
    "like",
    "mute",
    "ok",
    "one",
    "rock",
    "two",
    "two_inverted"
   # "three", 
   # "call"
]
#Data_DIR_PATH='Data'

Data_DIR_PATH='/content/drive/MyDrive/UGR/Data'

TRAIN_DATA_SIZE= 13860
VALID_DATA_SIZE=3960
TEST_DATA_SIZE=1980

#createFolders(Data_DIR_PATH, GESTURES_Names)
# Define the train and validate and test base directories
valid_path = os.path.join(Data_DIR_PATH, 'valid')
print(valid_path)
train_path = os.path.join(Data_DIR_PATH, 'train')
print(train_path)
test_path = os.path.join(Data_DIR_PATH, 'test')
print(test_path)

# Check the number of images for each class and set

# Directory with training five pictures
train_five_dir = os.path.join(train_path, 'five')
# Directory with training for pictures
train_four_dir = os.path.join(train_path, 'four')
# Directory with training klike pictures
train_like_dir = os.path.join(train_path, 'like')
# Directory with training mute pictures
train_mute_dir = os.path.join(train_path, 'mute')
# Directory with training ok pictures
train_ok_dir = os.path.join(train_path, 'ok')
# Directory with training one pictures
train_one_dir = os.path.join(train_path, 'one')
# Directory with training rock pictures
train_rock_dir = os.path.join(train_path, 'rock')
# Directory with training two pictures
train_two_dir = os.path.join(train_path, 'two')
# Directory with training two_inverted pictures
train_two_inverted_dir = os.path.join(train_path, 'two_inverted')


print(f"There are {len(os.listdir(train_five_dir))} images of five for training.\n")
print(f"There are {len(os.listdir(train_four_dir))} images of four for training.\n")
print(f"There are {len(os.listdir(train_like_dir))} images of like for training.\n")
print(f"There are {len(os.listdir(train_mute_dir))} images of mute for training.\n")
print(f"There are {len(os.listdir(train_ok_dir))} images of ok for training.\n")
print(f"There are {len(os.listdir(train_one_dir))} images of one for training.\n")
print(f"There are {len(os.listdir(train_rock_dir))} images of rock for training.\n")
print(f"There are {len(os.listdir(train_two_dir))} images of two for training.\n")
print(f"There are {len(os.listdir(train_two_inverted_dir))} images of two_inverted for training.\n")


# Directory with validation five pictures
validation_five_dir = os.path.join(valid_path, 'five')
# Directory with validation for pictures
validation_four_dir = os.path.join(valid_path, 'four')
# Directory with validation like pictures
validation_like_dir = os.path.join(valid_path, 'like')
# Directory with validation mute pictures
validation_mute_dir = os.path.join(valid_path, 'mute')
# Directory with validation ok pictures
validation_ok_dir = os.path.join(valid_path, 'ok')
# Directory with validation one pictures
validation_one_dir = os.path.join(valid_path, 'one')
# Directory with validation rock pictures
validation_rock_dir = os.path.join(valid_path, 'rock')
# Directory with validation two pictures
validation_two_dir = os.path.join(valid_path, 'two')
# Directory with validation two_inverted pictures
validation_two_inverted_dir = os.path.join(valid_path, 'two_inverted')

print(f"There are {len(os.listdir(validation_five_dir))} images of five for validation.\n")
print(f"There are {len(os.listdir(validation_four_dir))} images of four for validation.\n")
print(f"There are {len(os.listdir(validation_like_dir))} images of like for validation.\n")
print(f"There are {len(os.listdir(validation_mute_dir))} images of mute for validation.\n")
print(f"There are {len(os.listdir(validation_ok_dir))} images of ok for validation.\n")
print(f"There are {len(os.listdir(validation_one_dir))} images of one for validation.\n")
print(f"There are {len(os.listdir(validation_rock_dir))} images of rock for validation.\n")
print(f"There are {len(os.listdir(validation_two_dir))} images of two for validation.\n")
print(f"There are {len(os.listdir(validation_two_inverted_dir))} images of two_inverted for validation.\n")

# Directory with test five pictures
test_five_dir = os.path.join(test_path, 'five')
# Directory with test for pictures
test_four_dir = os.path.join(test_path, 'four')
# Directory with test like pictures
test_like_dir = os.path.join(test_path, 'like')
# Directory with test mute pictures
test_mute_dir = os.path.join(test_path, 'mute')
# Directory with test ok pictures
test_ok_dir = os.path.join(test_path, 'ok')
# Directory with test one pictures
test_one_dir = os.path.join(test_path, 'one')
# Directory with test rock pictures
test_rock_dir = os.path.join(test_path, 'rock')
# Directory with test two pictures
test_two_dir = os.path.join(test_path, 'two')
# Directory with test two_inverted pictures
test_two_inverted_dir = os.path.join(test_path, 'two_inverted')

print(f"There are {len(os.listdir(test_five_dir))} images of five for test.\n")
print(f"There are {len(os.listdir(test_four_dir))} images of four for test.\n")
print(f"There are {len(os.listdir(test_like_dir))} images of like for test.\n")
print(f"There are {len(os.listdir(test_mute_dir))} images of mute for test.\n")
print(f"There are {len(os.listdir(test_ok_dir))} images of ok for test.\n")
print(f"There are {len(os.listdir(test_one_dir))} images of one for test.\n")
print(f"There are {len(os.listdir(test_rock_dir))} images of rock for test.\n")
print(f"There are {len(os.listdir(test_two_dir))} images of two for test.\n")
print(f"There are {len(os.listdir(test_two_inverted_dir))} images of two_inverted for test.\n")

"""## Training and Validation Generators
Now that we have successfully organized the data in a way that can be easily fed to Keras' ImageDataGenerator, it is time for us to code the generators that will yield batches of images, both for training and validation. For this, we implement the dataGenerator_without_Aug function below. We also Implemented one with Augmentation dataGenerator_with_Aug

Something important to note is that the images in this dataset come in a variety of resolutions. Luckily, the flow_from_directory method allows us to standarize this by defining a tuple called target_size that will be used to convert each image to this target resolution. As we use the transfere learning with VGG16, we prepocess the image to preprocessing_input of the later and we will use a target_size of (224, 224).
"""

from keras.preprocessing.image import ImageDataGenerator
import tensorflow as  tf

def dataGenerator_without_Aug(preprocess_input,  train_path, valid_path, test_path, IMAGE_SIZE, BS, Gestures_Names):
    train_batches= ImageDataGenerator(preprocessing_function=preprocess_input) 
    train_gen=train_batches.flow_from_directory(directory=train_path, 
                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),
                                    classes=Gestures_Names,
                                    batch_size=BS)
                                    
    valid_batches= ImageDataGenerator(preprocessing_function=preprocess_input) 
    valid_gen=valid_batches.flow_from_directory(directory=valid_path, 
                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),
                                    classes=Gestures_Names,
                                    batch_size=BS)
    test_batches= ImageDataGenerator(preprocessing_function=preprocess_input)  
    test_gen=test_batches.flow_from_directory(directory=test_path, 
                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),
                                    classes=Gestures_Names,
                                    batch_size=BS)
    return train_gen, valid_gen, test_gen


def dataGenerator_with_Aug(preprocess_input,  train_path, valid_path, test_path, IMAGE_SIZE, BS, Gestures_Names):
      #la méthode flow_from_directory vous permet de normaliser la variété de resolutions en définissant un tuple appelé target_size qui sera utilisé pour convertir chaque image à cette résolution cible.
    # Pass in the appropiate arguments to the flow_from_directory meth
    train_batches= ImageDataGenerator(preprocessing_function=preprocess_input,
                                    rotation_range = 40,
                                    width_shift_range = 0.2,
                                    height_shift_range = 0.2,
                                    shear_range = 0.2,
                                    zoom_range = 0.2,
                                    horizontal_flip = True) # It attempts to recreate lost information after a transformation like a shear) 
    train_gen= train_batches.flow_from_directory(directory=train_path, 
                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),
                                    classes=Gestures_Names,
                                    batch_size=BS)
                                    
    valid_batches= ImageDataGenerator(preprocessing_function=preprocess_input) 
    valid_gen=valid_batches.flow_from_directory(directory=valid_path, 
                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),
                                    classes=Gestures_Names,
                                    batch_size=BS)
    test_batches= ImageDataGenerator(preprocessing_function=preprocess_input)
    #ImageDataGenerator.flow_from_directory() creates a DirectoryIterator, which generates batches of normalized tensor image data from the respective data directories.  
    test_gen=test_batches.flow_from_directory(directory=test_path, 
                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),
                                    classes=Gestures_Names,
                                    batch_size=BS,
                                    shuffle=False) # THE TEST SET MUST BE FALSE
    return train_gen, valid_gen, test_gen

# Method for test that the numbers that we specify are taken into account
def testDataGeneratorMethod(train_batches, valid_batches, test_batches, nb_classes, TRAIN_DATA_SIZE, VALID_DATA_SIZE, TEST_DATA_SIZE ):
    assert train_batches.n == TRAIN_DATA_SIZE 
    assert valid_batches.n== VALID_DATA_SIZE
    assert test_batches.n== TEST_DATA_SIZE
    assert train_batches.num_classes== valid_batches.num_classes==test_batches.num_classes==nb_classes



TOTAL_TRAIN_DATA_SIZE= 13860
TOTAL_VALID_DATA_SIZE=3960
TOTAL_TEST_DATA_SIZE=1980

preprocess_input_vgg= tf.keras.applications.vgg16.preprocess_input

train_batches, valid_batches, test_batches= dataGenerator_without_Aug(preprocess_input_vgg,  train_path, valid_path, test_path, 224, 20, GESTURES_Names)
testDataGeneratorMethod(train_batches, valid_batches, test_batches, 9, TOTAL_TRAIN_DATA_SIZE, TOTAL_VALID_DATA_SIZE, TOTAL_TEST_DATA_SIZE )



def create_model_VGG16_Fine_Tuned_with_Activation_Layer(IMAGE_SIZE,NBClasses):
  
  
    """   Compiles a model integrated with VGG16 pretrained layers
    
    image size: tuple - the shape of input images (width, height)
    NBClasses: int - number of classes for the output layer
    # fine_tune: int - The number of pre-trained layers to unfreeze.
    #               If set to 0, all pretrained layers will freeze during training """

# Load the pre-trained weights you downloaded.
    vgg16_model=tf.keras.applications.vgg16.VGG16()
    model=Sequential()
# umpload the vgg 16 model without the last layer (the predictions layer)
    for layer in vgg16_model.layers[:-1]:
      model.add(layer)

  # Iterate over all of the layyers with our sequantial model and Freeze the weights of the layers.
    for layer in model.layers:
        layer.trainable = False
    last_output = model.output
    # Add a final softmax layer for classification
    classification_layer = tf.keras.layers.Dense(units=NBClasses, activation='softmax', name='prob_output')(last_output)

    # Append the dense network and the presition network to the base model
    model = tf.keras.models.Model(inputs=model.input, outputs=[classification_layer])

    return model


def create_model_VGG16_Fine_Tuned_with_3_Activation_Layer(IMAGE_SIZE,NBClasses):
  
  
    """   Compiles a model integrated with VGG16 pretrained layers
    
    image size: tuple - the shape of input images (width, height)
    NBClasses: int - number of classes for the output layer
    # fine_tune: int - The number of pre-trained layers to unfreeze.
    #               If set to 0, all pretrained layers will freeze during training """

# Load the pre-trained weights you downloaded.
    vgg16_model=tf.keras.applications.vgg16.VGG16()
    model=Sequential()
# umpload the vgg 16 model without the last layer (the predictions layer)
    for layer in vgg16_model.layers[:-1]:
      model.add(layer)

  # Iterate over all of the layyers with our sequantial model and Freeze the weights of the layers.
    for layer in model.layers:
        layer.trainable = False
    last_output = model.output
    x = Dense(1024, activation='relu')(last_output)
    x = Dropout(0.2)(x)
    x = Dense(1024, activation='relu')(x)
    x = Dropout(0.2)(x)
    # Add a final softmax layer for classification
    classification_layer = tf.keras.layers.Dense(units=NBClasses, activation='softmax', name='prob_output')(x)

    # Append the dense network and the presition network to the base model
    model = tf.keras.models.Model(inputs=model.input, outputs=[classification_layer])

    return model

"""# Model neutre"""

def create_model_VGG16_Fine_Tuned_without_freeze(IMAGE_SIZE,NBClasses):
  
# Load the pre-trained weights you downloaded.
    vgg16_model=tf.keras.applications.vgg16.VGG16(include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))
    last_output = vgg16_model.output
    x = Flatten()(last_output)
    x = Dense(1024, activation='relu')(x)
    x = Dropout(0.2)(x)
    x = Dense(1024, activation='relu')(x)
    x = Dropout(0.2)(x)
    # Add a final softmax layer for classification
    classification_layer = tf.keras.layers.Dense(units=NBClasses, activation='softmax', name='prob_output')(x)

    # Append the dense network and the presition network to the base model
    model = tf.keras.models.Model(inputs=last_output, outputs=[classification_layer])

    return model

model= create_model_VGG16_Fine_Tuned_with_Activation_Layer(224,9)
model.summary()
plot_model(model, to_file='/content/drive/MyDrive/Colab Notebooks/UGR/model.png', rankdir='LR')
adam = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0)
#1e-3
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])





from tensorflow.keras.utils import plot_model


"""#Optimization

*   alpha. Also referred to as the learning rate or step size. The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial learning before the rate is updated. Smaller values (e.g. 1.0E-5) slow learning right down during training
*  beta1. The exponential decay rate for the first moment estimates (e.g. 0.9)
* beta2. The exponential decay rate for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient (e.g. NLP and computer vision problems).
* epsilon. Is a very small number to prevent any division by zero in the implementation (e.g. 10E-8).
"""

def diagrams(history):
    #-----------------------------------------------------------
# Retrieve a list of list results on training and test data
# sets for each training epoch
#-----------------------------------------------------------
    acc=history.history['accuracy']
    val_acc=history.history['val_accuracy']
    loss=history.history['loss']
    val_loss=history.history['val_loss']
    epochs=range(len(acc)) # Get number of epochs

#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
    plt.figure()
    plt.subplot(2,1,1)
    plt.plot(epochs, acc, 'red', label="Training Accuracy")
    plt.plot(epochs, val_acc, 'blue', label="Validation Accuracy")
    plt.legend()
    plt.title('Training and validation accuracy')
   
#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------
    plt.subplot(2,1,2)
    plt.plot(epochs, loss, 'bo', label="Training Loss")
    plt.plot(epochs, val_loss, 'b', label="Validation Loss")
    plt.title('Training and validation loss')
    plt.legend(loc=0)
    # Save the image
    plt.savefig('/content/drive/MyDrive/Colab Notebooks/UGR/figureHistory.png')
    plt.show()

"""Train The model and pass it in the ModelCheckpoint callback"""



checkpoint_path="/content/drive/MyDrive/Colab Notebooks/UGR/weights.best.large_Data.hdf5"
checkpoint_dir=os.path.dirname(checkpoint_path)
    # Learning Rate Reducer
learn_control = ReduceLROnPlateau(
                            monitor='val_accuracy',
                            patience=7,
#                           verbose=1,factor=0.2, 
                            min_lr=1e-7)
   
# CSVLoger logs epoch, acc, loss, val_acc, val_loss
los_csv=CSVLogger('/content/drive/MyDrive/Colab Notebooks/UGR/VGG16Net_large_logs.csv', separator=',', append=False )

early_stop = EarlyStopping(monitor='val_loss',
                           patience=10,
                           #restore_best_weights=True,
                           mode='min')

checkpoint = ModelCheckpoint(checkpoint_path, 
                                monitor='val_accuracy', 
                                verbose=1, 
                                save_best_only=True, 
                                mode='max')

"""#save the entire model to hdf5 file
Note: #we can save the entire model to a file contains the weights values, the model configuration, and even the optimizer configuration. This allos to chckepoint a model and resume training later from the exact same state

"""

"""#recreate the exact same model, including weights and optimizer."""

new_model=   model.load_weights(checkpoint_path)  #\\keras.models.load_model('my_model.h5')
new_model.summary()
loss, acc =  new_model.evaluate(test_baches)
print("Restored model, Accuracy:{:5.2f}".format(100*acc))

history=model.fit(x=train_batches,                     
          validation_data=valid_batches, 
          epochs=20, 
          verbose=2,
          callbacks=[ checkpoint, early_stop, los_csv])


model.save('my_model.h5')

#after Training
loss, acc =  new_model.evaluate(test_baches)
print("Restored model, Accuracy:{:5.2f}".format(100*acc))

with open('/content/drive/MyDrive/Colab Notebooks/UGR/weights/historyVGG16_large.txt', 'a+') as f:
    print(history.history, file=f)

print('All Done!')

diagrams(history)





"""#Visualizing the effct of the convolutions"""

